\documentclass{article}
\usepackage[yyyymmdd]{datetime}
\usepackage[nottoc]{tocbibind}
\usepackage{xurl}
\usepackage{array}
\usepackage[inline,shortlabels]{enumitem}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage{colortbl}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\newtheoremstyle{problemstyle}{3pt}{3pt}{\normalfont}{}{\bfseries}{\normalfont\bfseries:}{.5em}{}
\theoremstyle{problemstyle}
\newmdtheoremenv[
  linewidth=1pt,
  linecolor=RoyalBlue,
  backgroundcolor=RoyalBlue!10,
  roundcorner=5pt,
  innertopmargin=6pt,
  innerbottommargin=6pt,
  innerleftmargin=6pt,
  innerrightmargin=6pt,
  nobreak=true
]{problem}{Problem}

% Example
\newmdtheoremenv[
  linewidth=1pt,
  linecolor=ForestGreen,
  backgroundcolor=ForestGreen!10,
  roundcorner=5pt,
  nobreak=true
]{example}{Example}

% Theorem
\newmdtheoremenv[
  linewidth=1pt,
  linecolor=BrickRed,
  backgroundcolor=BrickRed!10,
  roundcorner=5pt,
  nobreak=true
]{theorem}{Theorem}

% Remark
\newmdtheoremenv[
  linewidth=1pt,
  linecolor=Goldenrod,
  backgroundcolor=Goldenrod!10,
  roundcorner=5pt,
  nobreak=true
]{remark}{Remark}

\newmdtheoremenv[
  linewidth=1pt,
  linecolor=Goldenrod,
  backgroundcolor=Goldenrod!10,
  roundcorner=5pt,
  nobreak=true
  ]{definition}{Definition}

% Solution
\newenvironment{solution}{%
  \begin{mdframed}[linewidth=0.8pt,linecolor=Gray,backgroundcolor=Gray!5,roundcorner=5pt, nobreak=true]%
  \noindent\textbf{Solution.}%
}{%
\hfill $ \diamond $ 
  \end{mdframed}%
}

\usepackage{listings}
\usepackage[utf8]{inputenc}
\setlength{\parindent}{0pt} % Don't indent new paragraphs
\setlength{\headheight}{24pt} 
% Make header with name and date etc.
\usepackage{fancyhdr}
\lhead{Pedro D. Llerenas\\M\'etodos Num\'ericos I}
\rhead{\today\\Tarea II}
\thispagestyle{fancy}

\newcommand{\Z}{\mathbb Z}
\newcommand{\Q}{\mathbb Q}
\newcommand{\R}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\N}{\mathbb N}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\DeclareMathOperator{\Var}{\mathbf{Var}}
\DeclareMathOperator{\E}{\mathbf{E}}

\begin{document}
	
\section{Distribuciones condicionales}\label{sec:distribuciones_condicionales} % (fold)
 Before, we wrote
 \[
  P(X\in S)
 \]
 Now, we will write 
 \[
   P(X\in S|A) \coloneqq \frac{P(\{w:X(w)\in S\}\cap A)}{P(A)}
 \]

En general, 
\[
  X=(X_1, \dots, X_d)
\]

\[
  P(X_i\in A | X_j \in B) = \displaystyle\frac{P(X_i)\in A, X_j \in B}{P(X_j \in B)}
\]

\subsection{Independence of random variable}

X, Y are independent random variables random variables if 
\begin{align*}
  \forall A,B: P(x\in A|y\in B) = P(x\in A)\\
  \forall A,B: P(y\in B|x\in A) = P(y\in B)\\
  \forall A,B: P(x\in A,y\in B) = P(x\in A)P(y\in B)
\end{align*}

% section Distribuciones condicionales (end)	

\begin{definition}
  We say that 
  \begin{displaymath}
    X \sim Y
  \end{displaymath}
  
  if the distribution of $ X $ equals the distribution of $ Y $.
\end{definition}

\subsection{Recap}

\begin{definition}
  Let $ X $ be a discrete random variable. We define de \textit{expected value} of $ X $ as
  \begin{displaymath}
    E[X] = \sum x \cdot P(X=x)
  \end{displaymath}
  
\end{definition}
We note that
\[
  \sum (x-E[X])\cdot P(X=x) = \sum x\cdot P(X = x) - E[X]\sum P(X=x) = 0
\]

\begin{definition}
  For any $ g:X \to Y$,
  \begin{displaymath}
    E[g(X)] = \sum g(x)P(x=X)
  \end{displaymath}
  
\end{definition}

\begin{theorem}
  $ E $ is linear.
  \begin{displaymath}
    E(\alpha X_1 + \beta X_2) = \alpha E[X_1] + \beta E[X_2]
  \end{displaymath}
  
\end{theorem}
\begin{proof}
  
  \begin{align*}
    E(\alpha X_1 + \beta X_2) &= \sum_{(x_1, x_2)} (\alpha x_1 + \beta x_2)P(X=(x_1,x_2))\\
                              &= \sum_{x_1}\sum_{x_2} (\alpha x_1 +\beta x_2)P(x_1=X_1, x_2=X_2)\\
                              &= \sum_{x_1}\sum_{x_2}\alpha x_1P(x_1=X_1, x_2=X_2) + \sum_{x_1}\sum_{x_2} \beta x_2P(x_1=X_1, x_2=X_2)\\
                              &= \sum_{x_1} \alpha x_1 P(x_1=X_1) + \sum_{x_2}\beta x_2P(x_2=X_2)
  \end{align*}
  
\end{proof}

\begin{theorem}
  If $ X_1, X_2 $ are independent random variables, then
  \[
    \E[X_1X_2] = \E[X_1]\E[X_2].
  \]
  
\end{theorem}
\begin{proof}
  \begin{align*}
    \E[X_1X_2] &= \displaystyle\sum_{x_1} \sum_{x_2} x_1x_2P(X_1=x_1, X_2=x_2) \\ 
               &= \sum_{x_1}x_1P(X_1=x_1)\sum_{x_2}x_2P(X_2=x_2) \\
               &= \E[X_1]\E[X_2]
  \end{align*}
\end{proof}

THe total probability law tells us that

\begin{align*}
  \E[X] &= \displaystyle\sum \E[X|Y=y]P(Y=y)\\
  &= \sum x P(X=x|Y=y)
\end{align*}

\begin{definition}
  For $ X\in \R $,
  \begin{align*}
    Var(X) = \E[(X-\E[X])^2] 
  \end{align*}
\end{definition}

\begin{remark}
  \begin{align*}
    \E[(X-\E X)^2] &= \E[X^2 - 2X\E[X] + \E[X]^2]\\
                   &= \E[X^2] - \E[2X\E[X]] + \E[\E[X]^2]\\
                   &= \E[X^2] - 2E[X]^2 + E[X]^2\\
                   &= \E[X^2] - \E[X]^2.
  \end{align*}
\end{remark}

\begin{theorem}
  For any random variable $ X\in \R^n $,
  \[
    \Var(X) \geq 0
  \]
 \[
   \Var(aX) = a^2 \Var(X),
 \] 
\end{theorem}

Given an event $ A $, how can we quantify the level of surprise this event may cause?

\begin{definition}[Entropy]
  Given an event $ A $, we define the its entropy as
  \[
    \mathcal E(A) \coloneqq-\sum \log(P(X=x)) P(X=x)
  \]
\end{definition}

\begin{theorem}
  For any event $ A $ ,
  $ \mathcal E(A) \geq 0 $.
\end{theorem}

\begin{problem}
  Let $ X $ be a random variable. Define $ Y\sim X $, independent of $ X $. Find $ P(X\neq Y) $.
\end{problem}
\begin{proof}
  We first notice that 
  \begin{align*}
    P(X=Y) &= \sum_y P(X=Y|Y=y)P(Y=y) \\
  &= \sum_y P(X=y|Y=y)P(Y=y) \\
  &= \sum_y P(X=y)P(Y=y) \\
  &= \sum_y P(X=y)^2
  \end{align*}
  \[
  \]
  therefore, 
  \[
    P(X\neq Y) = 1- P(X=Y) = 1-\sum_y P(X=y)^2 = \mathcal E_{gini}(X)
  \]
\end{proof}

\subsection{Distribution examples}

\begin{example}[Bernoulli]
The \textit{Bernoulli} distribution $ X\in \{0,1\} $ with
\[
  P(X=1) = \theta\qquad P(X=0) = 1-\theta
\]
then we have
\[
  \E[X] = \theta,
\]
and 
\[
  \Var X = \theta(1-\theta).
\]
\end{example}

\begin{example}[Binomial]
  The binomial distribution for $ X \in \{0,1,2,\dots,n\} $, $ X\sim Bin(\theta, n) $ arises from $ Y_i\sim Bern(\theta) $ independent
  \[
    X = \sum_{i=1}^n Y_i.
  \]
  We must then have
  \begin{align*}
    P(X=x) = {n\choose x} \theta^x (1-\theta)^{n-x}.
  \end{align*}
  One can prove this by counting the number of 1's or 0's in each n-string of experiments. We can thus see that
  \begin{align*}
    \E[X] = \E\left[\sum Y_i\right] = \sum \E[Y_i] = n\theta.
  \end{align*}
  \begin{align}
    \Var(X) = \Var(\sum Y_i) = \sum \Var(Y_i) = n\theta(1-\theta).
  \end{align}
\end{example}

\begin{example}[Geometric]
  The geometric distribution $ X\sim Geo(\theta) $ arises from given $ Y_i \sim Bern(\theta) $ and independent with $ X\in \{1,2,\dots\} $, then
  \begin{align}
    P(X=x) = \theta(1-\theta)^{x-1}
  \end{align}
  and 
  \begin{align*}
    \E[X] = \sum x \theta(1-\theta)^{x-1} = \frac{1}{\theta}
  \end{align*}
  
\end{example}

\begin{example}[Uniform]
  The uniform distribution $ X\sim Unif(\{1,2,3,\dots, n\}) $ has $ P(X=x) = n^{-1} $ and
  \[
    \E[X] = \displaystyle\frac{n+1}{2}
  \]
\end{example}


\section{Aplications}\label{sec:aplications} % (fold)
\subsection{Algorithm comlpexity}
Suppose we have a program 

% section Aplications (end)



\end{document}
